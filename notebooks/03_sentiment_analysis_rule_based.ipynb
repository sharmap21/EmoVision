{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d170b69",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with VADER & TextBlob\n",
    "\n",
    "This notebook implements rule-based sentiment analysis using:\n",
    "- **VADER** - Valence Aware Dictionary and sEntiment Reasoner, optimized for social media\n",
    "- **TextBlob** - Lexicon-based sentiment analysis library\n",
    "\n",
    "Performance evaluation is conducted against ground truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e1c801",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af970866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Sentiment Analysis libraries\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d5dc2",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data\n",
    "\n",
    "Loading the cleaned dataset and creating a balanced sample for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8960b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "df = pd.read_csv('dataset/cleaned_tweets.csv')\n",
    "\n",
    "print(f'Dataset loaded: {len(df):,} tweets')\n",
    "print(f'\\nColumns: {list(df.columns)}')\n",
    "print(f'\\nSentiment Distribution:')\n",
    "print(df['sentiment_label'].value_counts())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e0086c",
   "metadata": {},
   "source": [
    "## 3. Create Sample Dataset\n",
    "\n",
    "For faster testing and comparison, we'll create a balanced sample of 100,000 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972fdee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a balanced sample\n",
    "SAMPLE_SIZE = 50000  # 50k tweets per sentiment (100k total)\n",
    "\n",
    "df_sample = pd.concat([\n",
    "    df[df['sentiment'] == 0].sample(n=SAMPLE_SIZE, random_state=42),\n",
    "    df[df['sentiment'] == 1].sample(n=SAMPLE_SIZE, random_state=42)\n",
    "]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f'Sample created: {len(df_sample):,} tweets')\n",
    "print(f'\\nSample Sentiment Distribution:')\n",
    "print(df_sample['sentiment_label'].value_counts())\n",
    "print(f'\\nBalance: {(df_sample[\"sentiment_label\"].value_counts() / len(df_sample) * 100).round(2).to_dict()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d7f5c0",
   "metadata": {},
   "source": [
    "## 4. VADER Sentiment Analysis\n",
    "\n",
    "VADER is specifically designed for social media text and provides:\n",
    "- Positive score\n",
    "- Negative score  \n",
    "- Neutral score\n",
    "- Compound score (normalized between -1 to +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5685226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VADER\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_vader_sentiment(text):\n",
    "    \"\"\"\n",
    "    Get VADER sentiment scores for text.\n",
    "    Returns compound score and classification.\n",
    "    \"\"\"\n",
    "    scores = vader_analyzer.polarity_scores(text)\n",
    "    \n",
    "    # Classification based on compound score\n",
    "    if scores['compound'] >= 0.05:\n",
    "        sentiment = 1  # Positive\n",
    "    elif scores['compound'] <= -0.05:\n",
    "        sentiment = 0  # Negative\n",
    "    else:\n",
    "        # For neutral, classify based on which is higher: pos or neg\n",
    "        sentiment = 1 if scores['pos'] > scores['neg'] else 0\n",
    "    \n",
    "    return scores['compound'], sentiment\n",
    "\n",
    "print('VADER analyzer initialized!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b96581b",
   "metadata": {},
   "source": [
    "### 4.1 Apply VADER to Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003dde81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Analyzing sentiment with VADER...')\n",
    "print('This may take a few minutes...')\n",
    "\n",
    "# Apply VADER to original text (works better with original text than cleaned)\n",
    "vader_results = df_sample['text_original'].progress_apply(get_vader_sentiment)\n",
    "df_sample['vader_compound'] = vader_results.apply(lambda x: x[0])\n",
    "df_sample['vader_sentiment'] = vader_results.apply(lambda x: x[1])\n",
    "df_sample['vader_label'] = df_sample['vader_sentiment'].map({0: 'Negative', 1: 'Positive'})\n",
    "\n",
    "print('\\nVADER analysis completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8897de8",
   "metadata": {},
   "source": [
    "### 4.2 VADER Results Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ea811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample results\n",
    "print('Sample VADER Results:')\n",
    "print('='*100)\n",
    "\n",
    "sample_display = df_sample[['text_original', 'sentiment_label', 'vader_compound', 'vader_label']].head(10)\n",
    "for idx, row in sample_display.iterrows():\n",
    "    match = 'Match' if row['sentiment_label'] == row['vader_label'] else 'Mismatch'\n",
    "    print(f\"\\n[{match}] Tweet: {row['text_original'][:80]}...\")\n",
    "    print(f\"   True: {row['sentiment_label']:8s} | VADER: {row['vader_label']:8s} (Score: {row['vader_compound']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fbb79c",
   "metadata": {},
   "source": [
    "## 5. TextBlob Sentiment Analysis\n",
    "\n",
    "TextBlob provides:\n",
    "- Polarity: Float from -1 (negative) to +1 (positive)\n",
    "- Subjectivity: Float from 0 (objective) to 1 (subjective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea12784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_textblob_sentiment(text):\n",
    "    \"\"\"\n",
    "    Get TextBlob sentiment for text.\n",
    "    Returns polarity and classification.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        blob = TextBlob(text)\n",
    "        polarity = blob.sentiment.polarity\n",
    "        \n",
    "        # Classification based on polarity\n",
    "        sentiment = 1 if polarity > 0 else 0\n",
    "        \n",
    "        return polarity, sentiment\n",
    "    except:\n",
    "        return 0.0, 0\n",
    "\n",
    "print('TextBlob analyzer ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f71513",
   "metadata": {},
   "source": [
    "### 5.1 Apply TextBlob to Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80656e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Analyzing sentiment with TextBlob...')\n",
    "print('This may take a few minutes...')\n",
    "\n",
    "# Apply TextBlob to original text\n",
    "textblob_results = df_sample['text_original'].progress_apply(get_textblob_sentiment)\n",
    "df_sample['textblob_polarity'] = textblob_results.apply(lambda x: x[0])\n",
    "df_sample['textblob_sentiment'] = textblob_results.apply(lambda x: x[1])\n",
    "df_sample['textblob_label'] = df_sample['textblob_sentiment'].map({0: 'Negative', 1: 'Positive'})\n",
    "\n",
    "print('\\nTextBlob analysis completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8cf9a",
   "metadata": {},
   "source": [
    "### 5.2 TextBlob Results Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample results\n",
    "print('Sample TextBlob Results:')\n",
    "print('='*100)\n",
    "\n",
    "sample_display = df_sample[['text_original', 'sentiment_label', 'textblob_polarity', 'textblob_label']].head(10)\n",
    "for idx, row in sample_display.iterrows():\n",
    "    match = 'Match' if row['sentiment_label'] == row['textblob_label'] else 'Mismatch'\n",
    "    print(f\"\\n[{match}] Tweet: {row['text_original'][:80]}...\")\n",
    "    print(f\"   True: {row['sentiment_label']:8s} | TextBlob: {row['textblob_label']:8s} (Polarity: {row['textblob_polarity']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e0639f",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Let's evaluate both models using various metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35905d4",
   "metadata": {},
   "source": [
    "### 6.1 VADER Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f665f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for VADER\n",
    "y_true = df_sample['sentiment']\n",
    "y_pred_vader = df_sample['vader_sentiment']\n",
    "\n",
    "vader_accuracy = accuracy_score(y_true, y_pred_vader)\n",
    "vader_precision = precision_score(y_true, y_pred_vader)\n",
    "vader_recall = recall_score(y_true, y_pred_vader)\n",
    "vader_f1 = f1_score(y_true, y_pred_vader)\n",
    "vader_roc_auc = roc_auc_score(y_true, df_sample['vader_compound'])\n",
    "\n",
    "print('VADER SENTIMENT ANALYZER - PERFORMANCE METRICS')\n",
    "print('='*70)\n",
    "print(f'\\nAccuracy:  {vader_accuracy:.4f} ({vader_accuracy*100:.2f}%)')\n",
    "print(f'Precision: {vader_precision:.4f}')\n",
    "print(f'Recall:    {vader_recall:.4f}')\n",
    "print(f'F1-Score:  {vader_f1:.4f}')\n",
    "print(f'ROC-AUC:   {vader_roc_auc:.4f}')\n",
    "print('\\n' + '='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0f0ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report for VADER\n",
    "print('\\nVADER - Detailed Classification Report:')\n",
    "print(classification_report(y_true, y_pred_vader, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5558eda3",
   "metadata": {},
   "source": [
    "### 6.2 TextBlob Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37905cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for TextBlob\n",
    "y_pred_textblob = df_sample['textblob_sentiment']\n",
    "\n",
    "textblob_accuracy = accuracy_score(y_true, y_pred_textblob)\n",
    "textblob_precision = precision_score(y_true, y_pred_textblob)\n",
    "textblob_recall = recall_score(y_true, y_pred_textblob)\n",
    "textblob_f1 = f1_score(y_true, y_pred_textblob)\n",
    "textblob_roc_auc = roc_auc_score(y_true, df_sample['textblob_polarity'])\n",
    "\n",
    "print('TEXTBLOB SENTIMENT ANALYZER - PERFORMANCE METRICS')\n",
    "print('='*70)\n",
    "print(f'\\nAccuracy:  {textblob_accuracy:.4f} ({textblob_accuracy*100:.2f}%)')\n",
    "print(f'Precision: {textblob_precision:.4f}')\n",
    "print(f'Recall:    {textblob_recall:.4f}')\n",
    "print(f'F1-Score:  {textblob_f1:.4f}')\n",
    "print(f'ROC-AUC:   {textblob_roc_auc:.4f}')\n",
    "print('\\n' + '='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c0e8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report for TextBlob\n",
    "print('\\nTextBlob - Detailed Classification Report:')\n",
    "print(classification_report(y_true, y_pred_textblob, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4dfc18",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f39a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# VADER Confusion Matrix\n",
    "cm_vader = confusion_matrix(y_true, y_pred_vader)\n",
    "sns.heatmap(cm_vader, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "axes[0].set_title(f'VADER Confusion Matrix\\nAccuracy: {vader_accuracy:.2%}', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('True Label', fontsize=12)\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# TextBlob Confusion Matrix\n",
    "cm_textblob = confusion_matrix(y_true, y_pred_textblob)\n",
    "sns.heatmap(cm_textblob, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "axes[1].set_title(f'TextBlob Confusion Matrix\\nAccuracy: {textblob_accuracy:.2%}', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('True Label', fontsize=12)\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edc1efd",
   "metadata": {},
   "source": [
    "## 8. ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66cc486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curves\n",
    "fpr_vader, tpr_vader, _ = roc_curve(y_true, df_sample['vader_compound'])\n",
    "fpr_textblob, tpr_textblob, _ = roc_curve(y_true, df_sample['textblob_polarity'])\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_vader, tpr_vader, label=f'VADER (AUC = {vader_roc_auc:.4f})', linewidth=2)\n",
    "plt.plot(fpr_textblob, tpr_textblob, label=f'TextBlob (AUC = {textblob_roc_auc:.4f})', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Rule-Based Sentiment Analysis', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1399fcd",
   "metadata": {},
   "source": [
    "## 9. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedb8cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['VADER', 'TextBlob'],\n",
    "    'Accuracy': [vader_accuracy, textblob_accuracy],\n",
    "    'Precision': [vader_precision, textblob_precision],\n",
    "    'Recall': [vader_recall, textblob_recall],\n",
    "    'F1-Score': [vader_f1, textblob_f1],\n",
    "    'ROC-AUC': [vader_roc_auc, textblob_roc_auc]\n",
    "})\n",
    "\n",
    "print('\\nMODEL COMPARISON - RULE-BASED APPROACHES')\n",
    "print('='*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print('='*80)\n",
    "\n",
    "# Determine best model\n",
    "best_model = comparison_df.loc[comparison_df['F1-Score'].idxmax(), 'Model']\n",
    "print(f'\\nBest Performing Model: {best_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7707241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "vader_scores = [vader_accuracy, vader_precision, vader_recall, vader_f1, vader_roc_auc]\n",
    "textblob_scores = [textblob_accuracy, textblob_precision, textblob_recall, textblob_f1, textblob_roc_auc]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width/2, vader_scores, width, label='VADER', color='skyblue', edgecolor='black')\n",
    "bars2 = ax.bar(x + width/2, textblob_scores, width, label='TextBlob', color='lightcoral', edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Metrics', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Performance Comparison: VADER vs TextBlob', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562cf421",
   "metadata": {},
   "source": [
    "## 10. Sentiment Score Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233ee7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sentiment score distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# VADER - Negative vs Positive\n",
    "df_sample[df_sample['sentiment'] == 0]['vader_compound'].hist(\n",
    "    bins=50, ax=axes[0, 0], color='salmon', alpha=0.7, edgecolor='black'\n",
    ")\n",
    "axes[0, 0].set_title('VADER Scores - True Negative Tweets', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('VADER Compound Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(0, color='red', linestyle='--', linewidth=2, label='Threshold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "df_sample[df_sample['sentiment'] == 1]['vader_compound'].hist(\n",
    "    bins=50, ax=axes[0, 1], color='lightgreen', alpha=0.7, edgecolor='black'\n",
    ")\n",
    "axes[0, 1].set_title('VADER Scores - True Positive Tweets', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('VADER Compound Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].axvline(0, color='red', linestyle='--', linewidth=2, label='Threshold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# TextBlob - Negative vs Positive\n",
    "df_sample[df_sample['sentiment'] == 0]['textblob_polarity'].hist(\n",
    "    bins=50, ax=axes[1, 0], color='salmon', alpha=0.7, edgecolor='black'\n",
    ")\n",
    "axes[1, 0].set_title('TextBlob Scores - True Negative Tweets', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('TextBlob Polarity Score')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].axvline(0, color='red', linestyle='--', linewidth=2, label='Threshold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "df_sample[df_sample['sentiment'] == 1]['textblob_polarity'].hist(\n",
    "    bins=50, ax=axes[1, 1], color='lightgreen', alpha=0.7, edgecolor='black'\n",
    ")\n",
    "axes[1, 1].set_title('TextBlob Scores - True Positive Tweets', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('TextBlob Polarity Score')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].axvline(0, color='red', linestyle='--', linewidth=2, label='Threshold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5f525",
   "metadata": {},
   "source": [
    "## 11. Error Analysis\n",
    "\n",
    "Let's examine some misclassified examples to understand where these models struggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c3c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples for VADER\n",
    "vader_errors = df_sample[df_sample['sentiment'] != df_sample['vader_sentiment']].copy()\n",
    "print(f'VADER Misclassifications: {len(vader_errors):,} ({len(vader_errors)/len(df_sample)*100:.2f}%)')\n",
    "\n",
    "print('\\nSample VADER Misclassifications:')\n",
    "print('='*100)\n",
    "for idx, row in vader_errors.head(10).iterrows():\n",
    "    print(f\"\\nTweet: {row['text_original'][:100]}...\")\n",
    "    print(f\"True: {row['sentiment_label']:8s} | Predicted: {row['vader_label']:8s} | Score: {row['vader_compound']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce460826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples for TextBlob\n",
    "textblob_errors = df_sample[df_sample['sentiment'] != df_sample['textblob_sentiment']].copy()\n",
    "print(f'TextBlob Misclassifications: {len(textblob_errors):,} ({len(textblob_errors)/len(df_sample)*100:.2f}%)')\n",
    "\n",
    "print('\\nSample TextBlob Misclassifications:')\n",
    "print('='*100)\n",
    "for idx, row in textblob_errors.head(10).iterrows():\n",
    "    print(f\"\\nTweet: {row['text_original'][:100]}...\")\n",
    "    print(f\"True: {row['sentiment_label']:8s} | Predicted: {row['textblob_label']:8s} | Score: {row['textblob_polarity']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7415f7",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2659bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sample with predictions\n",
    "df_sample[[\n",
    "    'text_original', \n",
    "    'sentiment', \n",
    "    'sentiment_label',\n",
    "    'vader_compound', \n",
    "    'vader_sentiment', \n",
    "    'vader_label',\n",
    "    'textblob_polarity', \n",
    "    'textblob_sentiment', \n",
    "    'textblob_label'\n",
    "]].to_csv('dataset/rule_based_predictions.csv', index=False)\n",
    "\n",
    "print('Predictions saved to: dataset/rule_based_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d3a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save performance metrics\n",
    "comparison_df.to_csv('dataset/rule_based_performance.csv', index=False)\n",
    "print('Performance metrics saved to: dataset/rule_based_performance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cae1313",
   "metadata": {},
   "source": [
    "## 13. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696b0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*80)\n",
    "print('RULE-BASED SENTIMENT ANALYSIS SUMMARY')\n",
    "print('='*80)\n",
    "\n",
    "print('\\nDATASET:')\n",
    "print(f'  - Sample size: {len(df_sample):,} tweets')\n",
    "print(f'  - Negative tweets: {(df_sample[\"sentiment\"] == 0).sum():,}')\n",
    "print(f'  - Positive tweets: {(df_sample[\"sentiment\"] == 1).sum():,}')\n",
    "\n",
    "print('\\nMODELS EVALUATED:')\n",
    "print('  1. VADER Sentiment Analyzer')\n",
    "print('  2. TextBlob Sentiment Analyzer')\n",
    "\n",
    "print('\\nVADER PERFORMANCE:')\n",
    "print(f'  - Accuracy:  {vader_accuracy:.4f} ({vader_accuracy*100:.2f}%)')\n",
    "print(f'  - Precision: {vader_precision:.4f}')\n",
    "print(f'  - Recall:    {vader_recall:.4f}')\n",
    "print(f'  - F1-Score:  {vader_f1:.4f}')\n",
    "print(f'  - ROC-AUC:   {vader_roc_auc:.4f}')\n",
    "\n",
    "print('\\nTEXTBLOB PERFORMANCE:')\n",
    "print(f'  - Accuracy:  {textblob_accuracy:.4f} ({textblob_accuracy*100:.2f}%)')\n",
    "print(f'  - Precision: {textblob_precision:.4f}')\n",
    "print(f'  - Recall:    {textblob_recall:.4f}')\n",
    "print(f'  - F1-Score:  {textblob_f1:.4f}')\n",
    "print(f'  - ROC-AUC:   {textblob_roc_auc:.4f}')\n",
    "\n",
    "print(f'\\nBEST MODEL: {best_model}')\n",
    "print(f'  - Best F1-Score: {comparison_df[\"F1-Score\"].max():.4f}')\n",
    "\n",
    "print('\\nKEY INSIGHTS:')\n",
    "print('  - VADER performs better on social media text (designed for it)')\n",
    "print('  - Both models struggle with sarcasm and context')\n",
    "print('  - Rule-based approaches are fast but have accuracy limitations')\n",
    "\n",
    "print('\\nOUTPUT FILES:')\n",
    "print('  - dataset/rule_based_predictions.csv')\n",
    "print('  - dataset/rule_based_performance.csv')\n",
    "\n",
    "print('\\nAnalysis complete!')\n",
    "print('='*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
