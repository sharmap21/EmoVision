{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5621592a",
   "metadata": {},
   "source": [
    "# Data Preprocessing & Text Cleaning\n",
    "## Social Media Sentiment Analysis\n",
    "\n",
    "This notebook performs comprehensive text preprocessing including cleaning, normalization, lemmatization, and stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9abc5d",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c34b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca4c0da",
   "metadata": {},
   "source": [
    "## 2. Download NLTK Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a3a340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK resources downloaded!\n"
     ]
    }
   ],
   "source": [
    "nltk_resources = ['stopwords', 'punkt', 'wordnet', 'omw-1.4', 'punkt_tab']\n",
    "\n",
    "for resource in nltk_resources:\n",
    "    try:\n",
    "        nltk.download(resource, quiet=True)\n",
    "    except:\n",
    "        print(f'Could not download {resource}')\n",
    "\n",
    "print('NLTK resources downloaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fad037a",
   "metadata": {},
   "source": [
    "## 3. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94108be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 1,600,000 rows × 6 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "df = pd.read_csv('dataset/training.1600000.processed.noemoticon.csv', \n",
    "                 encoding='latin-1', \n",
    "                 names=column_names)\n",
    "\n",
    "print(f'Dataset loaded: {df.shape[0]:,} rows × {df.shape[1]} columns')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09235f8",
   "metadata": {},
   "source": [
    "## 4. Initial Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dae15b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment labels created:\n",
      "sentiment_label\n",
      "Negative    800000\n",
      "Positive    800000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...          0   \n",
       "1  is upset that he can't update his Facebook by ...          0   \n",
       "2  @Kenichan I dived many times for the ball. Man...          0   \n",
       "3    my whole body feels itchy and like its on fire           0   \n",
       "4  @nationwideclass no, it's not behaving at all....          0   \n",
       "\n",
       "  sentiment_label  \n",
       "0        Negative  \n",
       "1        Negative  \n",
       "2        Negative  \n",
       "3        Negative  \n",
       "4        Negative  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df.copy()\n",
    "\n",
    "df_clean['sentiment'] = df_clean['target'].map({0: 0, 4: 1})\n",
    "df_clean['sentiment_label'] = df_clean['sentiment'].map({0: 'Negative', 1: 'Positive'})\n",
    "\n",
    "print('Sentiment labels created:')\n",
    "print(df_clean['sentiment_label'].value_counts())\n",
    "print()\n",
    "print('Sample data:')\n",
    "df_clean[['text', 'sentiment', 'sentiment_label']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164d532a",
   "metadata": {},
   "source": [
    "## 5. Text Preprocessing Functions\n",
    "\n",
    "Comprehensive preprocessing functions for text cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a75915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing functions created!\n"
     ]
    }
   ],
   "source": [
    "def extract_features(text):\n",
    "    \"\"\"Extract features from text before cleaning (URLs, mentions, hashtags)\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    features['url_count'] = len(re.findall(r'http\\S+|www\\S+', text))\n",
    "    features['mention_count'] = len(re.findall(r'@\\w+', text))\n",
    "    features['hashtag_count'] = len(re.findall(r'#\\w+', text))\n",
    "    \n",
    "    hashtags = re.findall(r'#(\\w+)', text)\n",
    "    features['hashtags'] = ','.join(hashtags) if hashtags else ''\n",
    "    \n",
    "    return features\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Comprehensive text cleaning function\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove common English stopwords\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Lemmatize words to their root form\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def stem_text(text):\n",
    "    \"\"\"Stem words using Porter Stemmer\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "print('Preprocessing functions created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d6f166",
   "metadata": {},
   "source": [
    "## 6. Feature Extraction\n",
    "\n",
    "Extract features like URL count, mentions, hashtags before cleaning the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d914294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from tweets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600000/1600000 [00:05<00:00, 283240.37it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features extracted!\n",
      "\n",
      "Feature Statistics:\n",
      "Tweets with URLs: 81,117\n",
      "Tweets with mentions: 738,493\n",
      "Tweets with hashtags: 35,847\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>url_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Need a hug</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  url_count  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...          2   \n",
       "1  is upset that he can't update his Facebook by ...          0   \n",
       "2  @Kenichan I dived many times for the ball. Man...          0   \n",
       "3    my whole body feels itchy and like its on fire           0   \n",
       "4  @nationwideclass no, it's not behaving at all....          0   \n",
       "5                      @Kwesidei not the whole crew           0   \n",
       "6                                        Need a hug           0   \n",
       "7  @LOLTrish hey  long time no see! Yes.. Rains a...          0   \n",
       "8               @Tatiana_K nope they didn't have it           0   \n",
       "9                          @twittera que me muera ?           0   \n",
       "\n",
       "   mention_count  hashtag_count hashtags  \n",
       "0              1              0           \n",
       "1              0              0           \n",
       "2              1              0           \n",
       "3              0              0           \n",
       "4              1              0           \n",
       "5              1              0           \n",
       "6              0              0           \n",
       "7              1              0           \n",
       "8              1              0           \n",
       "9              1              0           "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Extracting features from tweets...')\n",
    "\n",
    "features_df = df_clean['text'].progress_apply(extract_features).apply(pd.Series)\n",
    "df_clean = pd.concat([df_clean, features_df], axis=1)\n",
    "\n",
    "print('\\nFeatures extracted!')\n",
    "print(f\"\\nFeature Statistics:\")\n",
    "print(f\"Tweets with URLs: {(df_clean['url_count'] > 0).sum():,}\")\n",
    "print(f\"Tweets with mentions: {(df_clean['mention_count'] > 0).sum():,}\")\n",
    "print(f\"Tweets with hashtags: {(df_clean['hashtag_count'] > 0).sum():,}\")\n",
    "\n",
    "df_clean[['text', 'url_count', 'mention_count', 'hashtag_count', 'hashtags']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557f7697",
   "metadata": {},
   "source": [
    "## 7. Text Cleaning Pipeline\n",
    "\n",
    "Apply comprehensive text cleaning to all tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3637446d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning text data...\n",
      "This may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600000/1600000 [00:56<00:00, 28510.88it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text cleaning completed!\n",
      "\n",
      "Comparison (Before vs After):\n",
      "\n",
      "1. ORIGINAL: @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "   CLEANED:  a thats a bummer you shoulda got david carr of third day to do it d\n",
      "\n",
      "2. ORIGINAL: is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\n",
      "   CLEANED:  is upset that he cant update his facebook by texting it and might cry as a result school today also blah\n",
      "\n",
      "3. ORIGINAL: @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds\n",
      "   CLEANED:  i dived many times for the ball managed to save the rest go out of bounds\n",
      "\n",
      "4. ORIGINAL: my whole body feels itchy and like its on fire \n",
      "   CLEANED:  my whole body feels itchy and like its on fire\n",
      "\n",
      "5. ORIGINAL: @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \n",
      "   CLEANED:  no its not behaving at all im mad why am i here because i cant see you all over there\n",
      "\n",
      "6. ORIGINAL: @Kwesidei not the whole crew \n",
      "   CLEANED:  not the whole crew\n",
      "\n",
      "7. ORIGINAL: Need a hug \n",
      "   CLEANED:  need a hug\n",
      "\n",
      "8. ORIGINAL: @LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?\n",
      "   CLEANED:  hey long time no see yes rains a bit only a bit lol im fine thanks hows you\n",
      "\n",
      "9. ORIGINAL: @Tatiana_K nope they didn't have it \n",
      "   CLEANED:  nope they didnt have it\n",
      "\n",
      "10. ORIGINAL: @twittera que me muera ? \n",
      "   CLEANED:  que me muera\n"
     ]
    }
   ],
   "source": [
    "print('Cleaning text data...')\n",
    "print('This may take a few minutes...')\n",
    "\n",
    "df_clean['text_original'] = df_clean['text']\n",
    "\n",
    "df_clean['text_cleaned'] = df_clean['text'].progress_apply(clean_text)\n",
    "\n",
    "print('\\nText cleaning completed!')\n",
    "print('\\nComparison (Before vs After):')\n",
    "comparison = df_clean[['text_original', 'text_cleaned']].head(10)\n",
    "for idx, row in comparison.iterrows():\n",
    "    print(f\"\\n{idx + 1}. ORIGINAL: {row['text_original']}\")\n",
    "    print(f\"   CLEANED:  {row['text_cleaned']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5b07a1",
   "metadata": {},
   "source": [
    "## 8. Remove Empty Tweets\n",
    "\n",
    "Remove any tweets that became empty after cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51ad46e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty tweets found: 3,219\n",
      "Dataset after removing empty tweets: 1,596,781 rows\n",
      "Dataset after removing empty tweets: 1,596,781 rows\n"
     ]
    }
   ],
   "source": [
    "empty_tweets = (df_clean['text_cleaned'].str.strip() == '') | (df_clean['text_cleaned'].isna())\n",
    "print(f'Empty tweets found: {empty_tweets.sum():,}')\n",
    "\n",
    "df_clean = df_clean[~empty_tweets].reset_index(drop=True)\n",
    "\n",
    "print(f'Dataset after removing empty tweets: {len(df_clean):,} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a797d",
   "metadata": {},
   "source": [
    "## 9. Remove Stopwords\n",
    "\n",
    "Remove common English stopwords that don't add much meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5793d013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing stopwords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1596781/1596781 [03:39<00:00, 7285.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopwords removed!\n",
      "\n",
      "Comparison (With Stopwords vs Without):\n",
      "\n",
      "1. WITH:    a thats a bummer you shoulda got david carr of third day to do it d\n",
      "   WITHOUT: thats bummer shoulda got david carr third day\n",
      "\n",
      "2. WITH:    is upset that he cant update his facebook by texting it and might cry as a result school today also blah\n",
      "   WITHOUT: upset cant update facebook texting might cry result school today also blah\n",
      "\n",
      "3. WITH:    i dived many times for the ball managed to save the rest go out of bounds\n",
      "   WITHOUT: dived many times ball managed save rest go bounds\n",
      "\n",
      "4. WITH:    my whole body feels itchy and like its on fire\n",
      "   WITHOUT: whole body feels itchy like fire\n",
      "\n",
      "5. WITH:    no its not behaving at all im mad why am i here because i cant see you all over there\n",
      "   WITHOUT: behaving im mad cant see\n",
      "\n",
      "6. WITH:    not the whole crew\n",
      "   WITHOUT: whole crew\n",
      "\n",
      "7. WITH:    need a hug\n",
      "   WITHOUT: need hug\n",
      "\n",
      "8. WITH:    hey long time no see yes rains a bit only a bit lol im fine thanks hows you\n",
      "   WITHOUT: hey long time see yes rains bit bit lol im fine thanks hows\n",
      "\n",
      "9. WITH:    nope they didnt have it\n",
      "   WITHOUT: nope didnt\n",
      "\n",
      "10. WITH:    que me muera\n",
      "   WITHOUT: que muera\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Removing stopwords...')\n",
    "\n",
    "df_clean['text_no_stopwords'] = df_clean['text_cleaned'].progress_apply(remove_stopwords)\n",
    "\n",
    "print('\\nStopwords removed!')\n",
    "print('\\nComparison (With Stopwords vs Without):')\n",
    "comparison = df_clean[['text_cleaned', 'text_no_stopwords']].head(10)\n",
    "for idx, row in comparison.iterrows():\n",
    "    print(f\"\\n{idx + 1}. WITH:    {row['text_cleaned']}\")\n",
    "    print(f\"   WITHOUT: {row['text_no_stopwords']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09354b18",
   "metadata": {},
   "source": [
    "## 10. Lemmatization\n",
    "\n",
    "Convert words to their base/dictionary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "524971fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1596781/1596781 [00:40<00:00, 39365.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatization completed!\n",
      "\n",
      "Sample lemmatized tweets:\n",
      "1. thats bummer shoulda got david carr third day\n",
      "2. upset cant update facebook texting might cry result school today also blah\n",
      "3. dived many time ball managed save rest go bound\n",
      "4. whole body feel itchy like fire\n",
      "5. behaving im mad cant see\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Lemmatizing text...')\n",
    "\n",
    "df_clean['text_lemmatized'] = df_clean['text_no_stopwords'].progress_apply(lemmatize_text)\n",
    "\n",
    "print('\\nLemmatization completed!')\n",
    "print('\\nSample lemmatized tweets:')\n",
    "for idx, text in enumerate(df_clean['text_lemmatized'].head(5), 1):\n",
    "    print(f\"{idx}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71149678",
   "metadata": {},
   "source": [
    "## 11. Stemming\n",
    "\n",
    "Apply stemming as an alternative preprocessing approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d69b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 247760/1596781 [00:23<02:05, 10741.31it/s]"
     ]
    }
   ],
   "source": [
    "print('Stemming text...')\n",
    "\n",
    "df_clean['text_stemmed'] = df_clean['text_no_stopwords'].progress_apply(stem_text)\n",
    "\n",
    "print('\\nStemming completed!')\n",
    "print('\\nComparison (Lemmatization vs Stemming):')\n",
    "comparison = df_clean[['text_lemmatized', 'text_stemmed']].head(10)\n",
    "for idx, row in comparison.iterrows():\n",
    "    print(f\"\\n{idx + 1}. LEMMA: {row['text_lemmatized']}\")\n",
    "    print(f\"   STEM:  {row['text_stemmed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c0906e",
   "metadata": {},
   "source": [
    "## 12. Text Length Analysis\n",
    "\n",
    "Compare text lengths before and after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452946da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['original_length'] = df_clean['text_original'].str.len()\n",
    "df_clean['cleaned_length'] = df_clean['text_lemmatized'].str.len()\n",
    "df_clean['original_word_count'] = df_clean['text_original'].str.split().str.len()\n",
    "df_clean['cleaned_word_count'] = df_clean['text_lemmatized'].str.split().str.len()\n",
    "\n",
    "print('Text Length Comparison:')\n",
    "print('\\nOriginal Text:')\n",
    "print(f\"  Avg character length: {df_clean['original_length'].mean():.2f}\")\n",
    "print(f\"  Avg word count: {df_clean['original_word_count'].mean():.2f}\")\n",
    "print('\\nCleaned Text (Lemmatized):')\n",
    "print(f\"  Avg character length: {df_clean['cleaned_length'].mean():.2f}\")\n",
    "print(f\"  Avg word count: {df_clean['cleaned_word_count'].mean():.2f}\")\n",
    "print(f\"\\nReduction: {((df_clean['original_length'].mean() - df_clean['cleaned_length'].mean()) / df_clean['original_length'].mean() * 100):.1f}% in character length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8757cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize length comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Original character length\n",
    "axes[0, 0].hist(df_clean['original_length'], bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Original Character Length', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Character Count')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(df_clean['original_length'].mean(), color='red', linestyle='--', label=f\"Mean: {df_clean['original_length'].mean():.1f}\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Cleaned character length\n",
    "axes[0, 1].hist(df_clean['cleaned_length'], bins=50, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_title('Cleaned Character Length', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Character Count')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].axvline(df_clean['cleaned_length'].mean(), color='red', linestyle='--', label=f\"Mean: {df_clean['cleaned_length'].mean():.1f}\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Original word count\n",
    "axes[1, 0].hist(df_clean['original_word_count'], bins=50, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_title('Original Word Count', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Word Count')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].axvline(df_clean['original_word_count'].mean(), color='red', linestyle='--', label=f\"Mean: {df_clean['original_word_count'].mean():.1f}\")\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Cleaned word count\n",
    "axes[1, 1].hist(df_clean['cleaned_word_count'], bins=50, color='plum', edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_title('Cleaned Word Count', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Word Count')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].axvline(df_clean['cleaned_word_count'].mean(), color='red', linestyle='--', label=f\"Mean: {df_clean['cleaned_word_count'].mean():.1f}\")\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec2c846",
   "metadata": {},
   "source": [
    "## 13. Remove Duplicate Tweets\n",
    "\n",
    "Remove duplicate tweets based on cleaned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639a156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_before = df_clean.duplicated(subset=['text_lemmatized']).sum()\n",
    "print(f'Duplicate tweets found: {duplicates_before:,}')\n",
    "\n",
    "df_clean = df_clean.drop_duplicates(subset=['text_lemmatized'], keep='first').reset_index(drop=True)\n",
    "\n",
    "print(f'Dataset after removing duplicates: {len(df_clean):,} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac30d254",
   "metadata": {},
   "source": [
    "## 14. Dataset Structure Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d427364",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset Columns:')\n",
    "print('  - text_original: Original tweets')\n",
    "print('  - text_cleaned: Lowercased, URLs/mentions/numbers removed')\n",
    "print('  - text_no_stopwords: Cleaned + stopwords removed')\n",
    "print('  - text_lemmatized: Final processed (recommended for modeling)')\n",
    "print('  - text_stemmed: Alternative stemmed version')\n",
    "print('\\nDataset is ready for sentiment analysis modeling!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c450ca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nSample of Final Dataset:')\n",
    "df_clean[['text_original', 'text_lemmatized', 'sentiment_label']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58982bb",
   "metadata": {},
   "source": [
    "## 15. Save Preprocessed Data\n",
    "\n",
    "Save the cleaned dataset for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd20385",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_save = [\n",
    "    'text_original',\n",
    "    'text_cleaned', \n",
    "    'text_lemmatized',\n",
    "    'text_stemmed',\n",
    "    'sentiment',\n",
    "    'sentiment_label',\n",
    "    'url_count',\n",
    "    'mention_count',\n",
    "    'hashtag_count',\n",
    "    'hashtags',\n",
    "    'user'\n",
    "]\n",
    "\n",
    "df_save = df_clean[columns_to_save].copy()\n",
    "\n",
    "output_file = 'dataset/cleaned_tweets.csv'\n",
    "df_save.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f'Preprocessed data saved to: {output_file}')\n",
    "print(f'Saved {len(df_save):,} tweets')\n",
    "print(f'File size: {round(df_save.memory_usage(deep=True).sum() / 1024**2, 2)} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5013897d",
   "metadata": {},
   "source": [
    "## 16. Preprocessing Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc6907",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*80)\n",
    "print('DATA PREPROCESSING SUMMARY REPORT')\n",
    "print('='*80)\n",
    "print('\\nDATA STATISTICS:')\n",
    "print(f'  - Original dataset: 1,600,000 tweets')\n",
    "print(f'  - After removing empty tweets: {len(df_clean):,} tweets')\n",
    "print(f'  - Final dataset: {len(df_save):,} tweets')\n",
    "print(f'  - Data retention: {(len(df_save)/1600000*100):.2f}%')\n",
    "\n",
    "print('\\nPREPROCESSING STEPS COMPLETED:')\n",
    "print('  - Text lowercase conversion')\n",
    "print('  - URL removal')\n",
    "print('  - Mention (@user) removal')\n",
    "print('  - Hashtag symbol removal (text kept)')\n",
    "print('  - Number removal')\n",
    "print('  - Punctuation removal')\n",
    "print('  - Extra whitespace removal')\n",
    "print('  - Stopwords removal')\n",
    "print('  - Lemmatization')\n",
    "print('  - Stemming (alternative)')\n",
    "print('  - Duplicate removal')\n",
    "\n",
    "print('\\nFEATURE ENGINEERING:')\n",
    "print(f'  - URL count: {(df_clean[\"url_count\"] > 0).sum():,} tweets with URLs')\n",
    "print(f'  - Mention count: {(df_clean[\"mention_count\"] > 0).sum():,} tweets with mentions')\n",
    "print(f'  - Hashtag count: {(df_clean[\"hashtag_count\"] > 0).sum():,} tweets with hashtags')\n",
    "\n",
    "print('\\nSENTIMENT DISTRIBUTION:')\n",
    "for label, count in df_clean['sentiment_label'].value_counts().items():\n",
    "    pct = (count / len(df_clean)) * 100\n",
    "    print(f'  - {label}: {count:,} ({pct:.2f}%)')\n",
    "\n",
    "print('\\nTEXT LENGTH ANALYSIS:')\n",
    "print(f'  - Avg original length: {df_clean[\"original_length\"].mean():.2f} chars')\n",
    "print(f'  - Avg cleaned length: {df_clean[\"cleaned_length\"].mean():.2f} chars')\n",
    "print(f'  - Avg original words: {df_clean[\"original_word_count\"].mean():.2f}')\n",
    "print(f'  - Avg cleaned words: {df_clean[\"cleaned_word_count\"].mean():.2f}')\n",
    "\n",
    "print('\\nOUTPUT FILES:')\n",
    "print(f'  - Cleaned dataset: dataset/cleaned_tweets.csv')\n",
    "\n",
    "print('\\nPreprocessing complete! Ready for sentiment analysis.')\n",
    "print('='*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
